# Import necessary libraries and modules
import matplotlib  # Import the matplotlib library for plotting
matplotlib.use('Agg')  # Set the backend to 'Agg' to suppress GUI-based plots when running in a non-interactive environment
import pandas as pd  # Import pandas for data manipulation
import numpy as np  # Import numpy for numerical operations
import matplotlib.pyplot as plt  # Import the pyplot module from matplotlib for plotting

# Import relevant scikit-learn functionalities
from sklearn.feature_selection import SelectKBest, f_classif  # Import feature selection functions
from sklearn.preprocessing import StandardScaler  # Import StandardScaler for feature scaling
from sklearn.tree import DecisionTreeClassifier, plot_tree  # Import DecisionTreeClassifier for decision trees
from sklearn.svm import SVC  # Import Support Vector Classifier (SVM)
from sklearn.neighbors import KNeighborsClassifier  # Import K-Nearest Neighbors Classifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, LeaveOneOut  # Import functions for model evaluation and cross-validation
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report  # Import metrics for evaluation

def load_and_clean_data(filepath):
    """Load CSV data and perform basic cleaning."""
    # Read the CSV file into a DataFrame
    data_df = pd.read_csv(filepath)
    # Remove any duplicate rows from the data
    data_df.drop_duplicates(inplace=True)
    # Replace any missing values (NaN) with zeros
    data_df.fillna(0, inplace=True)
    return data_df

def prepare_data(df):
    """Extract features and labels from the dataframe."""
    # Extract 'class' column as labels for the model
    data_labels = df['class'].copy()
    # Convert categorical columns ('packer' and 'packer_type') into dummy/indicator variables
    data_dummies_df = pd.get_dummies(df, columns=['packer', 'packer_type'])
    # Drop 'class' column to get feature matrix
    data_features = data_dummies_df.drop(columns='class', axis=1)
    return data_features, data_labels

def feature_selection_and_transformation(features, labels):
    """Perform feature selection and scaling."""
    # Use univariate feature selection method to get the top 16 features
    kbest_selector = SelectKBest(f_classif, k=15)
    top_features = kbest_selector.fit_transform(features, labels)
    # Standardize the feature set (mean=0 and variance=1)
    scaler = StandardScaler()
    standardized_features = scaler.fit_transform(top_features)
    return standardized_features

def plot_roc_curve(false_positive_rate, true_positive_rate, auc_value, classifier_name):
    """Generate and save the ROC curve for a classifier."""
    # Create a new figure for the plot
    plt.figure()
    # Plot the ROC curve
    plt.plot(false_positive_rate, true_positive_rate, color='pink', lw=2, label=f'ROC curve (area = {auc_value:.2f})')
    # Plot a diagonal dashed line (represents random guessing)
    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
    # Define axis limits
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    # Add axis labels and a title
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {classifier_name}')
    plt.legend(loc="lower right")
    # Save the figure to a file
    plt.savefig(f'roc_curve_{classifier_name}.png', dpi=300)
    print(f"ROC curve for {classifier_name} created and saved as roc_curve_{classifier_name}.png.")
    plt.close()

def evaluate_model(classifier, features, labels):
    """Train and evaluate a classifier, then plot its ROC curve."""
    # Split the dataset into training (67%) and testing (33%) sets
    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33)
    # Train the classifier on the training set
    classifier.fit(train_features, train_labels)
    # Predict the labels for the test set
    predicted_labels = classifier.predict(test_features)
    # Compute the accuracy of the classifier
    model_accuracy = np.mean(predicted_labels == test_labels)
    
    # If the classifier can provide probabilities, plot the ROC curve
    if hasattr(classifier, "predict_proba"):
        # Get the predicted probabilities for the positive class
        predicted_probs = classifier.predict_proba(test_features)
        false_pos_rate, true_pos_rate, _ = roc_curve(test_labels, predicted_probs[:, 1])
        roc_auc_score = auc(false_pos_rate, true_pos_rate)
        plot_roc_curve(false_pos_rate, true_pos_rate, roc_auc_score, classifier.__class__.__name__)
        
    # Calculate and display confusion matrix and classification report
    conf_matrix = confusion_matrix(test_labels, predicted_labels)
    class_report = classification_report(test_labels, predicted_labels)
    print(f"Confusion Matrix for {classifier.__class__.__name__}:\n{conf_matrix}")
    print(f"Classification Report for {classifier.__class__.__name__}:\n{class_report}")
    
    return model_accuracy

def main():
    # Define the path to the CSV file
    data_filepath = 'ClaMP_Integrated-5184.csv'
    # Load and clean the data from the CSV
    clean_data_df = load_and_clean_data(data_filepath)
    # Prepare the features and labels from the cleaned data
    extracted_features, extracted_labels = prepare_data(clean_data_df)
    # Select the top features and standardize them
    final_features = feature_selection_and_transformation(extracted_features, extracted_labels)

 # Define a list of classifiers to evaluate
    classifier_models = [
        ('Decision Tree', DecisionTreeClassifier(max_depth=3)),
        ('SVM', SVC(probability=True)),
        ('KNN', KNeighborsClassifier())
    ]
    
    # Train, test, and evaluate each classifier
    for model_name, model_classifier in classifier_models:
        accuracy_result = evaluate_model(model_classifier, final_features, extracted_labels)
        print(f"{model_name} Accuracy with Holdout: {accuracy_result:.2f}")
    
    # Define cross-validation methods
    cross_val_methods = [('Stratified K-Fold', StratifiedKFold(n_splits=10)), 
                         ('Leave One Out', LeaveOneOut())]
    
    # Evaluate classifier performance using cross-validation
    for cv_method_name, cv_method in cross_val_methods:
        for model_name, model_classifier in classifier_models:
            if cv_method_name == "Leave One Out":
                print(f"Starting Leave One Out cross-validation for {model_name}...")
            cross_val_results = cross_val_score(model_classifier, final_features, extracted_labels, cv=cv_method)
            print(f"{model_name} Accuracy with {cv_method_name}: {cross_val_results.mean():.2f} (+/- {cross_val_results.std() * 2:.2f})")
            if cv_method_name == "Leave One Out":
                print(f"Completed Leave One Out cross-validation for {model_name}.")

    # Create bar graphs for classifier performance
    plt.figure(figsize=(12, 6))
    for model_name, results in classifier_results.items():
        plt.bar(model_name, np.mean(results), yerr=np.std(results) * 2, label=model_name)

    plt.xlabel('Classifiers')
    plt.ylabel('Accuracy')
    plt.title('Classifier Performance Comparison')
    plt.legend()
    plt.tight_layout()
    plt.savefig('classifier_comparison.png', dpi=300)
    plt.close()

    # Create bar graphs for cross-validation performance
    plt.figure(figsize=(12, 6))
    for cv_method_name, results in cv_results.items():
        plt.bar(cv_method_name, np.mean(results), yerr=np.std(results) * 2, label=cv_method_name)

    plt.xlabel('Cross-Validation Methods')
    plt.ylabel('Accuracy')
    plt.title('Cross-Validation Performance Comparison')
    plt.legend()
    plt.tight_layout()
    plt.savefig('crossval_comparison.png', dpi=300)
    plt.close()

    # Define the number of features to plot in the scatter plot
    num_features_to_plot = 4  
    feature_names = extracted_features.columns[:num_features_to_plot]
    plt.figure(figsize=(12, 8))
    for i in range(num_features_to_plot):
        for j in range(i + 1, num_features_to_plot):
            plt.subplot(num_features_to_plot - 1, num_features_to_plot - 1, i * (num_features_to_plot - 1) + j)
            plt.scatter(extracted_features.iloc[:, i], extracted_features.iloc[:, j], c=extracted_labels, cmap='viridis')
            plt.xlabel(feature_names[i])
            plt.ylabel(feature_names[j])
            plt.title(f'Scatter plot of {feature_names[i]} vs {feature_names[j]}')
    plt.tight_layout()
    plt.savefig('scatter_plot.png', dpi=300)
    plt.close()

    # Train a decision tree on the dataset and visualize it
    dt_classifier = DecisionTreeClassifier(max_depth=3).fit(final_features, extracted_labels)
    plt.figure(figsize=(20, 10))
    plot_tree(dt_classifier, filled=True, feature_names=[f'Feature {i}' for i in range(final_features.shape[1])], class_names=['0', '1'])
    plt.title('Decision Tree Visualization')
    plt.savefig('decision_tree_visualization.png', dpi=300)
    plt.close()

if __name__ == "__main__":
    main()
